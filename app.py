import pandas as pd
import numpy as np
from flask import Flask, jsonify, send_from_directory, request
import os
import sqlite3
import tensorflow as tf
import joblib
from datetime import datetime

# --- 0. ì„¤ì • ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ---
app = Flask(__name__, 
            static_folder='earthquake-risk-heatmap/dist/assets', 
            static_url_path='/assets')

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì›ë³¸ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ API í˜¸ì¶œ ì‹œ ì†ë„ í–¥ìƒ
model = None
scaler = None
raw_df = None

DB_PATH = 'features_database.db'
RAW_DATA_PATH = 'japan_earthquake_data_raw.csv'
MODEL_PATH = 'earthquake_lstm_model.h5'
SCALER_PATH = 'earthquake_scaler.gz'
SEQUENCE_LENGTH = 12

# ì•± ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤í–‰ (Flask ê¶Œì¥ ë°©ì‹)
with app.app_context():
    try:
        print("AI ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        model = tf.keras.models.load_model(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        raw_df = pd.read_csv(RAW_DATA_PATH, parse_dates=['time'])
        print("âœ… ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ê²½ê³ : ì‹œì‘ ì‹œ í•„ìš”í•œ íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ({e})")

# --- 1. ì›¹í˜ì´ì§€ ì„œë¹™ ---
dist_folder_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                'earthquake-risk-heatmap', 'dist')
@app.route("/")
@app.route("/<path:path>")
def serve_vite_app(path=None):
    # Vite ë¹Œë“œ ê²°ê³¼ì¸ index.htmlì„ ì œê³µ
    if not os.path.exists(os.path.join(dist_folder_path, 'index.html')):
        return "Frontend not found. Please run 'npm run build' in the frontend directory.", 404
    return send_from_directory(dist_folder_path, 'index.html')

# --- 2. ì‹¤ì‹œê°„ ì˜ˆì¸¡ API (íƒ€ì„ë¨¸ì‹  ê¸°ëŠ¥) ---
@app.route("/api/predict_at")
def predict_at_date():
    # 1. ì‚¬ìš©ì ìš”ì²­ì—ì„œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (YYYY-MM í˜•ì‹)
    target_month_str = request.args.get('date')
    if not target_month_str:
        return jsonify({"error": "ë‚ ì§œë¥¼ 'YYYY-MM' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400
    
    # ëª¨ë¸ì´ë‚˜ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
    if model is None or scaler is None or raw_df is None:
        return jsonify({"error": "ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}), 503

    try:
        # 2. ì˜ˆì¸¡ì— í•„ìš”í•œ ê¸°ê°„ ê³„ì‚°
        target_month_end = pd.to_datetime(target_month_str + '-01').to_period('M').end_time
        start_period = target_month_end - pd.DateOffset(months=SEQUENCE_LENGTH - 1)
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ íŠ¹ì§• ë°ì´í„° ì¡°íšŒ
        conn = sqlite3.connect(f'file:{DB_PATH}?mode=ro', uri=True)
        query = f"""
            SELECT * FROM monthly_features 
            WHERE time BETWEEN '{start_period.strftime('%Y-%m-%d')}' AND '{target_month_end.strftime('%Y-%m-%d')}'
        """
        features_df = pd.read_sql_query(query, conn)
        conn.close()
        
        # 4. AI ì˜ˆì¸¡ ì‹¤í–‰
        features_to_scale = ['count', 'mean', 'count_3m_avg', 'count_6m_avg', 'count_12m_avg', 'b_value', 'days_since_last_quake']
        
        # ğŸš¨ ì—¬ê¸°ê°€ ì¶”ê°€ëœ ë¶€ë¶„: ì˜ˆì¸¡ì— ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì˜ ë¹ˆ ê°’ì„ 0ìœ¼ë¡œ ì±„ìš´ë‹¤.
        features_df[features_to_scale] = features_df[features_to_scale].fillna(0)
        
        predict_data = features_df.groupby('grid_id').filter(lambda x: len(x) == SEQUENCE_LENGTH)
        grid_ids_to_predict = predict_data['grid_id'].unique()
        
        X_predict, latest_features_for_grids = [], {}
        for grid_id in grid_ids_to_predict:
            grid_data = predict_data[predict_data['grid_id'] == grid_id].sort_values('time')
            X_predict.append(scaler.transform(grid_data[features_to_scale]))
            latest_features_for_grids[grid_id] = grid_data.iloc[-1]

        if not X_predict:
            return jsonify({"forecasts": [], "actual_quakes": []}), 200
            
        X_predict = np.array(X_predict)
        predictions = model.predict(X_predict, verbose=0)

        # 5. ì‹¤ì œ ë°œìƒí•œ ì§€ì§„ ë°ì´í„° ì¡°íšŒ
        actual_quakes_df = raw_df[raw_df['time'].dt.strftime('%Y-%m') == target_month_str]
        actual_quakes = actual_quakes_df[['latitude', 'longitude', 'magnitude', 'depth']].to_dict('records')
        
        # 6. ìµœì¢… ê²°ê³¼ ì¡°í•©
        GRID_SIZE = 0.5; LAT_MIN = 24; LON_MIN = 122
        forecast_list = []
        for i, grid_id in enumerate(grid_ids_to_predict):
            prob = predictions[i][0] * 100
            details_row = latest_features_for_grids[grid_id]
            lat_idx, lon_idx = map(int, grid_id.split('_'))
            south, west = LAT_MIN + lat_idx * GRID_SIZE, LON_MIN + lon_idx * GRID_SIZE
            forecast_list.append({
                "grid_id": grid_id, "bounds": [[south, west], [south + GRID_SIZE, west + GRID_SIZE]],
                "risk_probability": round(float(prob), 2),
                "details": {"count": int(details_row['count']), "mean": round(details_row['mean'], 2),
                            "count_12m_avg": round(details_row['count_12m_avg'], 2), "b_value": round(details_row['b_value'], 2),
                            "days_since_last_quake": int(details_row['days_since_last_quake'])}
            })
            
        response_data = {
            "analyzed_month": target_month_str, # 'update_time' ëŒ€ì‹  'ë¶„ì„ëœ ì›”'ì„ ëª…í™•íˆ ì „ë‹¬
            "forecasts": forecast_list,
            "actual_quakes": actual_quakes
        }
        return jsonify(response_data)

    except Exception as e:
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë” ìƒì„¸í•œ ë¡œê¹…ì´ í•„ìš”
        print(f"Error during prediction: {e}")
        return jsonify({"error": f"íƒ€ì„ë¨¸ì‹  ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

# --- 3. ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80, debug=False)