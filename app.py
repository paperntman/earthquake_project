import pandas as pd
import numpy as np
from flask import Flask, jsonify, send_from_directory, request
import os
import sqlite3
import tensorflow as tf
import joblib
from datetime import datetime

# --- 0. ì„¤ì • ë° ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ì´ˆê¸°í™” ---
app = Flask(__name__, 
            static_folder='earthquake-risk-heatmap/dist/assets', 
            static_url_path='/assets')

# ì „ì—­ ë³€ìˆ˜ë¡œ ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì›ë³¸ ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ë¡œë“œí•˜ì—¬ API í˜¸ì¶œ ì‹œ ì†ë„ í–¥ìƒ
model = None
scaler = None
raw_df = None

DB_PATH = 'features_database.db'
RAW_DATA_PATH = 'japan_earthquake_data_raw.csv'
MODEL_PATH = 'earthquake_lstm_model.h5'
SCALER_PATH = 'earthquake_scaler.gz'
SEQUENCE_LENGTH = 12

# ì•± ì»¨í…ìŠ¤íŠ¸ ì•ˆì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤í–‰ (Flask ê¶Œì¥ ë°©ì‹)
with app.app_context():
    try:
        print("AI ëª¨ë¸ê³¼ ìŠ¤ì¼€ì¼ëŸ¬ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
        model = tf.keras.models.load_model(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        raw_df = pd.read_csv(RAW_DATA_PATH, parse_dates=['time'])
        print("âœ… ëª¨ë¸, ìŠ¤ì¼€ì¼ëŸ¬, ì›ë³¸ ë°ì´í„° ë¡œë“œ ì™„ë£Œ.")
    except Exception as e:
        print(f"âš ï¸ ê²½ê³ : ì‹œì‘ ì‹œ í•„ìš”í•œ íŒŒì¼ì„ ë¡œë“œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ({e})")

# --- 1. ì›¹í˜ì´ì§€ ì„œë¹™ ---
dist_folder_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), 
                                'earthquake-risk-heatmap', 'dist')
@app.route("/")
@app.route("/<path:path>")
def serve_vite_app(path=None):
    # Vite ë¹Œë“œ ê²°ê³¼ì¸ index.htmlì„ ì œê³µ
    if not os.path.exists(os.path.join(dist_folder_path, 'index.html')):
        return "Frontend not found. Please run 'npm run build' in the frontend directory.", 404
    return send_from_directory(dist_folder_path, 'index.html')

# --- 2. ì‹¤ì‹œê°„ ì˜ˆì¸¡ API (íƒ€ì„ë¨¸ì‹  ê¸°ëŠ¥) ---
@app.route("/api/predict_at")
def predict_at_date():
    # 1. ì‚¬ìš©ì ìš”ì²­ì—ì„œ ë‚ ì§œ ê°€ì ¸ì˜¤ê¸° (YYYY-MM í˜•ì‹)
    target_month_str = request.args.get('date')
    if not target_month_str:
        return jsonify({"error": "ë‚ ì§œë¥¼ 'YYYY-MM' í˜•ì‹ìœ¼ë¡œ ì…ë ¥í•´ì£¼ì„¸ìš”."}), 400
    
    # ëª¨ë¸ì´ë‚˜ ë°ì´í„°ê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì—ëŸ¬ ë°˜í™˜
    if model is None or scaler is None or raw_df is None:
        return jsonify({"error": "ì„œë²„ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}), 503

    try:
        # 2. ì˜ˆì¸¡ì— í•„ìš”í•œ ê¸°ê°„ ê³„ì‚°
        prediction_base_date = pd.to_datetime(target_month_str + '-01') - pd.DateOffset(days=1)
        
        # ì´ ë‚ ì§œê°€ ì´ì œ íŠ¹ì§•ì„ ê°€ì ¸ì˜¤ëŠ” ê¸°ê°„ì˜ ë§ˆì§€ë§‰ì´ ë¨
        target_month_end = prediction_base_date 
        
        # LSTM ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ê³ ë ¤í•˜ì—¬ ì‹œì‘ ê¸°ê°„ ê³„ì‚°
        start_period = target_month_end - pd.DateOffset(months=SEQUENCE_LENGTH - 1)
        
        # 3. ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í•„ìš”í•œ íŠ¹ì§• ë°ì´í„° ì¡°íšŒ (ê¸°ê°„ì´ í•œ ë‹¬ ì•ë‹¹ê²¨ì§)
        conn = sqlite3.connect(f'file:{DB_PATH}?mode=ro', uri=True)
        query = f"""
            SELECT * FROM monthly_features 
            WHERE time BETWEEN '{start_period.strftime('%Y-%m-%d')}' AND '{target_month_end.strftime('%Y-%m-%d')}'
        """
        features_df = pd.read_sql_query(query, conn)
        conn.close()
        
        # 4. AI ì˜ˆì¸¡ ì‹¤í–‰
        features_to_scale = ['count', 'mean', 'count_3m_avg', 'count_6m_avg', 'count_12m_avg', 'b_value', 'days_since_last_quake']
        
        # ğŸš¨ ì—¬ê¸°ê°€ ì¶”ê°€ëœ ë¶€ë¶„: ì˜ˆì¸¡ì— ì‚¬ìš©í•  íŠ¹ì§•ë“¤ì˜ ë¹ˆ ê°’ì„ 0ìœ¼ë¡œ ì±„ìš´ë‹¤.
        features_df[features_to_scale] = features_df[features_to_scale].fillna(0)
        
        predict_data = features_df.groupby('grid_id').filter(lambda x: len(x) == SEQUENCE_LENGTH)
        grid_ids_to_predict = predict_data['grid_id'].unique()
        
        X_predict, latest_features_for_grids = [], {}
        for grid_id in grid_ids_to_predict:
            grid_data = predict_data[predict_data['grid_id'] == grid_id].sort_values('time')
            X_predict.append(scaler.transform(grid_data[features_to_scale]))
            latest_features_for_grids[grid_id] = grid_data.iloc[-1]

        if not X_predict:
            return jsonify({"forecasts": [], "actual_quakes": []}), 200
            
        X_predict = np.array(X_predict)
        predictions = model.predict(X_predict, verbose=0)

        # 5. ì‹¤ì œ ë°œìƒí•œ ì§€ì§„ ë°ì´í„° ì¡°íšŒ
        actual_quakes_df = raw_df[raw_df['time'].dt.strftime('%Y-%m') == target_month_str]
        actual_quakes = actual_quakes_df[['latitude', 'longitude', 'magnitude', 'depth']].to_dict('records')
        
        # 6. ìµœì¢… ê²°ê³¼ ì¡°í•©
        GRID_SIZE = 0.5; LAT_MIN = 24; LON_MIN = 122
        forecast_list = []
        for i, grid_id in enumerate(grid_ids_to_predict):
            prob = predictions[i][0] * 100
            details_row = latest_features_for_grids[grid_id]
            lat_idx, lon_idx = map(int, grid_id.split('_'))
            south, west = LAT_MIN + lat_idx * GRID_SIZE, LON_MIN + lon_idx * GRID_SIZE
            forecast_list.append({
                "grid_id": grid_id, "bounds": [[south, west], [south + GRID_SIZE, west + GRID_SIZE]],
                "risk_probability": round(float(prob), 2),
                "details": {"count": int(details_row['count']), "mean": round(details_row['mean'], 2),
                            "count_12m_avg": round(details_row['count_12m_avg'], 2), "b_value": round(details_row['b_value'], 2),
                            "days_since_last_quake": int(details_row['days_since_last_quake'])}
            })
            
        response_data = {
            "analyzed_month": target_month_str, # 'update_time' ëŒ€ì‹  'ë¶„ì„ëœ ì›”'ì„ ëª…í™•íˆ ì „ë‹¬
            "forecasts": forecast_list,
            "actual_quakes": actual_quakes
        }
        return jsonify(response_data)

    except Exception as e:
        # ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” ë” ìƒì„¸í•œ ë¡œê¹…ì´ í•„ìš”
        print(f"Error during prediction: {e}")
        return jsonify({"error": f"íƒ€ì„ë¨¸ì‹  ì˜ˆì¸¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}"}), 500

import json # íŒŒì¼ ìƒë‹¨ì— import ë˜ì–´ìˆëŠ”ì§€ í™•ì¸, ì—†ë‹¤ë©´ ì¶”ê°€

# --- ìƒˆë¡œìš´ API: ìµœì‹  ì˜ˆë³´ ë°ì´í„° ì „ì†¡ ---
LATEST_FORECAST_PATH = 'latest_forecast.json'

@app.route("/api/latest")
def get_latest_forecast():
    """
    ë¯¸ë¦¬ ê³„ì‚°ëœ ìµœì‹  ì˜ˆë³´ JSON íŒŒì¼ì„ ì½ì–´ ê·¸ëŒ€ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
    ë°ì´í„°ë² ì´ìŠ¤ ì¡°íšŒë‚˜ AI ì˜ˆì¸¡ ì—†ì´ ë§¤ìš° ë¹ ë¥´ê²Œ ë™ì‘í•©ë‹ˆë‹¤.
    """
    try:
        # with openìœ¼ë¡œ íŒŒì¼ì„ ì—´ë©´ ìë™ìœ¼ë¡œ ë‹«ì•„ì£¼ì–´ ì•ˆì „í•©ë‹ˆë‹¤.
        with open(LATEST_FORECAST_PATH, 'r', encoding='utf-8') as f:
            latest_data = json.load(f)
        
        # í”„ë¡ íŠ¸ì—”ë“œê°€ ì‚¬ìš©í•˜ëŠ” ë°ì´í„° í˜•ì‹ê³¼ ë§ì¶”ê¸° ìœ„í•´ í‚¤ ì´ë¦„ì„ ë³€ê²½í•©ë‹ˆë‹¤.
        # latest_forecast.jsonì˜ 'update_time' -> 'analyzed_month'
        # 'update_time'ì´ 'YYYY-MM-DD HH:MM:SS' í˜•ì‹ì´ë¯€ë¡œ 'YYYY-MM'ìœ¼ë¡œ ì˜ë¼ì¤ë‹ˆë‹¤.
        update_time_str = latest_data.get('update_time', '')
        if update_time_str:
            # datetime ê°ì²´ë¡œ ë³€í™˜í–ˆë‹¤ê°€ ë‹¤ì‹œ ì›í•˜ëŠ” í˜•ì‹ìœ¼ë¡œ í¬ë§·íŒ…í•©ë‹ˆë‹¤.
            analyzed_dt = datetime.strptime(update_time_str, '%Y-%m-%d %H:%M:%S')
            # í•´ë‹¹ ì›”ì˜ ì˜ˆì¸¡ì´ë¯€ë¡œ, í•´ë‹¹ ì›”ì„ ê¸°ì¤€ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
            # ì˜ˆ: 2024-05-15ì— ìƒì„±ë˜ì—ˆìœ¼ë©´, 2024ë…„ 6ì›” ì˜ˆì¸¡ì´ë¯€ë¡œ 2024-06ì´ ë¶„ì„ ëŒ€ìƒ ì›”.
            # í•˜ì§€ë§Œ run_daily_forecast.py ë¡œì§ ìƒ ë§ˆì§€ë§‰ ë‹¬ì„ ê¸°ì¤€ìœ¼ë¡œ í•˜ë¯€ë¡œ,
            # ìƒì„±ëœ ë‚ ì§œì˜ ì›”ì„ ë¶„ì„ ì›”ë¡œ ë´ë„ ë¬´ë°©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœí•˜ê²Œ YYYY-MMìœ¼ë¡œ ìë¦…ë‹ˆë‹¤.
            latest_data['analyzed_month'] = analyzed_dt.strftime('%Y-%m')
        else:
            latest_data['analyzed_month'] = "N/A"
            
        # 'update_time' í‚¤ëŠ” í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì‚¬ìš©í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œê±°í•˜ê±°ë‚˜ ê·¸ëŒ€ë¡œ ë‘¬ë„ ë©ë‹ˆë‹¤.
        # ì¼ê´€ì„±ì„ ìœ„í•´ ì œê±°í•˜ëŠ” ê²ƒì´ ë” ê¹”ë”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
        if 'update_time' in latest_data:
            del latest_data['update_time']
            
        # í”„ë¡ íŠ¸ì—”ë“œì—ì„œ actual_quakes í‚¤ë¥¼ ê¸°ëŒ€í•˜ë¯€ë¡œ, ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•´ì¤ë‹ˆë‹¤.
        # ìµœì‹  ì˜ˆë³´ëŠ” 'ë¯¸ë˜'ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë¯€ë¡œ 'ì‹¤ì œ ë°œìƒ ì§€ì§„'ì€ ì•„ì§ ì—†ìŠµë‹ˆë‹¤.
        if 'actual_quakes' not in latest_data:
            latest_data['actual_quakes'] = []

        return jsonify(latest_data)
        
    except FileNotFoundError:
        # latest_forecast.json íŒŒì¼ì´ ì•„ì§ ìƒì„±ë˜ì§€ ì•Šì€ ê²½ìš°
        return jsonify({"error": "ìµœì‹  ì˜ˆë³´ ë°ì´í„°ê°€ ì•„ì§ ì¤€ë¹„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."}), 404
    except Exception as e:
        # íŒŒì¼ì€ ìˆìœ¼ë‚˜ JSON í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆê±°ë‚˜ ë‹¤ë¥¸ ì˜¤ë¥˜ê°€ ë°œìƒí•œ ê²½ìš°
        print(f"Error reading latest forecast file: {e}")
        return jsonify({"error": "ìµœì‹  ì˜ˆë³´ ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."}), 500

# --- 3. ì„œë²„ ì‹¤í–‰ ---
if __name__ == "__main__":
    app.run(host='0.0.0.0', port=80, debug=False)