# íŒŒì¼ ì´ë¦„: precompute_features.py
import pandas as pd
import numpy as np
from tqdm import tqdm
import sqlite3
from datetime import datetime

def calculate_b_value(magnitudes):
    """b-valueë¥¼ ê³„ì‚°í•˜ëŠ” í•¨ìˆ˜"""
    if len(magnitudes) < 30: return np.nan
    min_mag = 2.5
    return 1 / (np.log(10) * (np.mean(magnitudes) - min_mag))

def run_precomputation():
    """
    ëª¨ë“  ê¸°ê°„ì— ëŒ€í•œ ëª¨ë“  íŠ¹ì§•ì„ ë¯¸ë¦¬ ê³„ì‚°í•˜ì—¬
    SQLite ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ë©”ì¸ í•¨ìˆ˜.
    """
    print("### ëª¨ë“  ê¸°ê°„ì— ëŒ€í•œ íŠ¹ì§• ë°ì´í„° ì‚¬ì „ ê³„ì‚° ì‹œì‘ ###")
    
    # --- 1. ì›ë³¸ ë°ì´í„° ë¡œë“œ ---
    print("(1/5) ì›ë³¸ ë°ì´í„°ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤...")
    try:
        df = pd.read_csv("japan_earthquake_data_raw.csv", parse_dates=['time'])
    except FileNotFoundError:
        print("ì˜¤ë¥˜: 'japan_earthquake_data_raw.csv' íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
        return
        
    df['grid_id'] = (
        pd.cut(df['latitude'], bins=np.arange(24, 47.1, 0.5), labels=False, include_lowest=True).astype(str) + '_' +
        pd.cut(df['longitude'], bins=np.arange(122, 148.1, 0.5), labels=False, include_lowest=True).astype(str)
    )
    df.dropna(subset=['grid_id'], inplace=True)
    df = df.set_index('time').sort_index()

    # --- 2. ì›”ë³„ ê¸°ë³¸ íŠ¹ì§• ì§‘ê³„ ---
    print("(2/5) ì›”ë³„ ê¸°ë³¸ íŠ¹ì§•ì„ ì§‘ê³„í•©ë‹ˆë‹¤...")
    monthly_agg = df.groupby('grid_id').resample('ME')['magnitude'].agg(['count', 'mean']).fillna(0).reset_index()

    # --- 3. ì´ë™ í‰ê·  íŠ¹ì§• ìƒì„± ---
    print("(3/5) ì´ë™ í‰ê·  íŠ¹ì§•ì„ ìƒì„±í•©ë‹ˆë‹¤...")
    windows = [3, 6, 12]
    for w in windows:
        monthly_agg[f'count_{w}m_avg'] = monthly_agg.groupby('grid_id')['count'].transform(
            lambda x: x.rolling(window=w, min_periods=1).mean()
        )
    
    # --- 4. ê³¼í•™ì  íŠ¹ì§• ìƒì„± ---
    print("(4/5) ê³¼í•™ì  íŠ¹ì§•(b-value, ê²½ê³¼ì¼)ì„ ìƒì„±í•©ë‹ˆë‹¤... (ì‹œê°„ì´ ë§ì´ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    feature_rows = []
    for grid_id, group in tqdm(monthly_agg.groupby('grid_id'), desc="ê²©ìë³„ ê³„ì‚°"):
        grid_df_original = df[df['grid_id'] == grid_id]
        for _, row in group.iterrows():
            current_month_end = row['time']
            
            # b-value ê³„ì‚°
            start_date_1y = current_month_end - pd.DateOffset(years=1)
            past_1y_quakes = grid_df_original.loc[start_date_1y:current_month_end, 'magnitude']
            b_val = calculate_b_value(past_1y_quakes)

            # days_since_last_quake ê³„ì‚°
            quakes_before_now = grid_df_original.loc[:current_month_end]
            days_since = 9999 if quakes_before_now.empty else (current_month_end - quakes_before_now.index.max()).days
            
            new_row = row.to_dict()
            new_row['b_value'] = b_val
            new_row['days_since_last_quake'] = days_since
            feature_rows.append(new_row)

    final_features_df = pd.DataFrame(feature_rows)
    # ğŸš¨ ì—¬ê¸°ê°€ ìˆ˜ì •ëœ ë¶€ë¶„: b_valueê°€ NaNì´ì–´ë„ ë²„ë¦¬ì§€ ì•Šê³ , ê²½ê³¼ì¼ë§Œ ì±„ìš´ë‹¤!
    final_features_df['days_since_last_quake'].fillna(9999, inplace=True)
    
    final_features_df['time'] = final_features_df['time'].dt.strftime('%Y-%m-%d')
    
    # --- 5. ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥ ---
    DB_PATH = 'features_database.db'
    print(f"(5/5) ê³„ì‚°ëœ íŠ¹ì§• ë°ì´í„° {len(final_features_df)}ê°œë¥¼ ë°ì´í„°ë² ì´ìŠ¤ '{DB_PATH}'ì— ì €ì¥í•©ë‹ˆë‹¤...")
    conn = sqlite3.connect(DB_PATH)
    
    final_features_df.to_sql('monthly_features', conn, if_exists='replace', index=False)
    
    print("ë¹ ë¥¸ ì¡°íšŒë¥¼ ìœ„í•´ ë°ì´í„°ë² ì´ìŠ¤ ì¸ë±ìŠ¤ë¥¼ ìƒì„±í•©ë‹ˆë‹¤...")
    conn.execute("CREATE INDEX IF NOT EXISTS idx_grid_time ON monthly_features (grid_id, time)")
    conn.commit()
    conn.close()
    
    print(f"\nâœ… ëª¨ë“  íŠ¹ì§• ë°ì´í„°ê°€ '{DB_PATH}'ì— ì„±ê³µì ìœ¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ì´ì œ ì´ DB íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ app.pyì—ì„œ ì‹¤ì‹œê°„ ì˜ˆì¸¡ APIë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    run_precomputation()